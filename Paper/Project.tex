
\documentclass[journal]{IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%

  %\usepackage[pdftex]{graphicx}
  \usepackage{graphicx}
  \usepackage{caption}
  \usepackage{subcaption}
  \usepackage{amsmath}
  % declare the path(s) where your graphic files are
 \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[export]{adjustbox}
\usepackage{float}

\usepackage[section]{placeins}

% correct bad hyphenation here
\usepackage{fixltx2e}
 \usepackage{cite} 
\usepackage{url}


\usepackage{epstopdf}

\epstopdfDeclareGraphicsRule{.gif}{png}{.png}{convert gif:#1 png:\OutputFile}
\AppendGraphicsExtensions{.gif}

\setcounter{totalnumber}{5}
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Project: 3D Pose Estimation from 2D Projections}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Richard (Alex) Showalter-Bucher\\Benjamin Yu}


\maketitle

\section{Introduction}
In this project, we explored the application of machine learning to the problem of estimating an object's 3D position and orientation, known as a pose, from a 2D projection. Being able to accurately perform this estimation has applications in several disciplines including Computer Vision, Photogrammetry, and Robotics\cite{Robust_PNP}. A notable application is object tracking in virtual reality systems (VR), such as the HTC Vive and Oculus Rift. Both of these systems estimate 2D angular locations of reference points on the objects being tracked to determine the object's pose.

For a system like the HTC Vive, this measurement is done directly via time of arrival of laser sweeps on reference points which are photodiodes. (Fig \ref{fig:vive}) In the case of the Oculus Rift, a camera is used to identify predetermined reference points (IR LEDs) before estimating the 2D angular location. This is done by blinking each LED in a specific pattern which is observed by a camera. (Fig \ref{fig:oculus}) After the reference points are identified they are centroided to determine their angular location on the focal plane. Once the 2D angular locations are estimated a Perspective N-Point method is applied to estimate the objects pose. In the next subsection we discuss what this method is in more detail. 


\begin{figure}
	\hspace{-10 ex}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{vive}
		\caption{HTC Vive\cite{vive_image}}
		\label{fig:vive}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{oculus}
		\caption{Oculus Rift\cite{oculus_image}}
		\label{fig:oculus}
	\end{subfigure}
	\caption{Left: HTC Vive's laser sweep with reference points. Right: Oculus Rift's camera with reference points}
	\label{fig:Vive and Oculus}
\end{figure}


\subsection{Perspective N-Point (PnP) Method}
The problem of estimating an objects 3D pose from 2D projections of known reference points is referred to as the "Perspective N-Point" problem. (PnP) 
Common approaches to this problem involve directly or iteratively solving a system of equations\cite{EPnP}. While these approaches can be very precise with the well defined reference points (Minimum of 4 non co-planar points for a non-ambiguous solution), there are usually trade-offs between accuracy and compute time. Direct solvers of these system of equations can be performed computationally fast, but may not be robustly accurate. On the other hand, iterative methods tend to be slower but more robust. A well-formed hybrid of these solvers can be both fast and robustly accurate for well defined points. One of these methods was used to baseline our performance to compare with the results of our machine learning algorithms. This baseline method is discussed in more detail in the Baseline Performance EPnP section.



\subsection{Motivation for Applying Machine Learning}
We wanted to implement and test several machine learning techniques to see how they fair with handling the pose estimation problem. There were three different types of techniques we used: SVM Regression, Feed-Forward Neural Networks (FFNN), and Convolutional Neural Networks(CNN). 

We used two different datasets to train our models: raw RGB image data and 2D projections of geometric reference points from an object.
These data types are similar to the raw and processed data, respectively, of the Oculus Rift system. 

Using the reference point data, we were curious if we could match the performance of the PnP method and, if we could obtain similar performance using many points, determine if pose regression using machine learning was robust using less than 4 points (the limitation for non-ambiguous solutions using PnP). Due to the sparsity of this type of data, set we did not train a CNN with it; we tested this type of data only on the SVM and FFNN. 

For the raw image data, we were curious if we could train our models such that they could directly estimate the poses of objects without the need to perform multiple processing steps such as reference point identification and centroiding. If this method worked robustly, then tracked objects may be able to remove the need to explicitly define reference points which would allow for a more cost effective and general solution to the pose estimation problem. 
 
 We also note the use of PnP algorithms require that the reference points, no matter how their locations are measured, are only useful if their exact relative geometry to one another is known. That is, the PnP algorithms are given knowledge of what the object looks like. In the case our machine learning algorithms, the geometry of the objects are not specified, and must be learned during the training phase. The challenge of tracking objects would then be free of the problem of getting exact geometric measurements.
 
Unfortunately, we found that none of our techniques met the performance of the PnP.  We were hoping to get comparable results to PnP methods but found that the estimations from all data sets were significantly worse than the PnP. We discuss this in more detail in each techniques respective section.

In the next section we discuss our data set and how it was generated. 


\section{Data Set Generation}

To test our machine learning and baseline PnP algorithm we generated two data sets. Each dataset contains versions of a cube, a cone, and a sphere in 500 randomly generated poses. (So each dataset contains 1500 total different samples.) Each pose contains a 3D translation and then yaw, pitch, and roll rotations.

\subsection{Geometric Data Points}

The first data set consists of the projected 2D locations of geometric reference points from the various shapes. We defined a set of 3D points on the surfaces of the cube, the cone, and the sphere. These base shapes were centered around the origin of our coordinate system. Then, a randomly generated translation and rotation (a pose) was applied to the points. Finally, the points were projected onto a 2D image using a sixty degree field of view.(FOV)

The geometry of the world was set in a right-handed coordinate system. The camera is at the origin and points along the negative z-axis, so that the x-axis points to the right, and the the y-axis points up (from the point of view of the camera(Fig \ref{openglFrustum})).

The base geometry of each shape was chosen to to have a characteristic dimension of 1.0; the sphere had a diameter of 1.0, the cone had a diameter of 1.0 at its base, and a height of 1.0, the cube had sides of length 1.0. The rotations were allowed to vary uniformly throughout the space. Yaw varied between 0 and 360 degrees. Pitch varied between positive 90 and negative 90 degrees. Roll varied between 0 and 180 degrees. Those spans are enough to specify any 3D rotation. The x and y of the coordinates of the translation were uniformly distributed between -1.5 and +1.5. The z coordinated varied between -4.33 and -1.7320. This z range placed our shapes at a distance from the camera so that they took up roughly one fifth to a third of the total FOV. We note it was possible for a shape to be half off the image.

The math for applying our random pose, and then projecting our points onto a 2D image follow the vertex processing of OpenGL 1.0. We start with our points from the base shape. These are converted to homogeneous coordinates, which are 4-vectors of the form:

\begin{equation}
\vec r = (x, y, z, 1)^T
\end{equation}

\noindent Where $x$, $y$, and $z$ are the traditional 3D coordinates.
The benefit of this coordinate system is that both translations and rotations can be written as a matrix multiplication. We create one matrix for the translation, as well as for each of the yaw, pitch, and roll rotations. ($T$, $Y$, $P$, and $R$ matrices.) Finally, we calculate a projection matrix ($C$) which defines a frustum (Fig \ref{openglFrustum}) that represents the field of view of the camera.

\begin{figure}
	\hspace{-10 ex}

	\centering
	\includegraphics[width=.8\linewidth]{gl_projectionmatrix01}

	\caption{The frustum defined by the projection matrix, simulating the view of a camera. \cite{opengl_image}}
	\label{openglFrustum}
\end{figure}

We then calculate the final locations of the points:

\begin{equation}
\vec r_{posed} = C \cdot T \cdot Y \cdot P \cdot R \cdot \vec r
\end{equation}

We normalize these coordinates, by dividing each element by the 4th component of each vector. These normalized coordinates have the property that every point inside that frustum has x, y, and z coordinates in [-1, 1]. Anything else is out of the FOV.

We also calculated the occlusion of each of these points. The final points have z-coordinates, and by defining a set of triangles on the surface of each shape, it is possible to see if a point is blocked by some surface segment of the shape. In the end we record only the location of the projected points on the x-y plane and if the point was occluded.

**FIGURES OF SHAPES**

\subsection{Image Data Set}
Our second data set consisted of full bitmap images of posed cubes, cones, and spheres on a black background. We used the GLUT library to speed our creation of these scenes with OpenGL. Like before, we created 500 random poses of each of the three shapes. The math implemented is the same as in the geometric point case, but of course, OpenGL will render a full scene after transforming the shape vertices.

We applied a texture to our shapes. We chose a high-res version of the Earth from NASA public release. We figured this texture was interesting and also asymmetric. Lack of symmetry is beneficial since it's possible to determine which face of the cube you are observing; each face will have a unique part of the Earth on it.

We rendered 64x64 24-bit RGB images and saved them in BMP format. The images are saved unnormalized, but we applied normalization in our algorithms.
Samples are shown in Fig. \ref{openglCone}, \ref{openglCube}, and \ref{openglSphere}.

\begin{figure}[H]
	\hspace{-10 ex}

	\centering
	\includegraphics[width=.6\linewidth]{cone_450}

	\caption{A sample of a randomly posed cone in our image data set.}
	\label{openglCone}
\end{figure}

\begin{figure}[H]
	\hspace{-10 ex}

	\centering
	\includegraphics[width=.6\linewidth]{cube_301}

	\caption{A sample of a randomly posed cone in our image data set.}
	\label{openglCube}
\end{figure}

\begin{figure}[H]
	\hspace{-10 ex}

	\centering
	\includegraphics[width=.6\linewidth]{sphere_450}

	\caption{A sample of a randomly posed cone in our image data set.}
	\label{openglSphere}
\end{figure}

\section{Baseline Performance of EPnP}

 As a baseline to the performance of the 3D estimation, we used a method referred to as Efficient PnP (EPnP). The implementation of the solver used was included as part of a paper by Lepetit et al\cite{EPnP}. This method is a hybrid method between a direct and iterative solver. In first part of the algorithm, a weighted sum of four virtual points are calculated from N observed reference points. These four virtual points are then used in the direct solver which gives an initial guess of the solution. This guess is then fed into a Gauss-Newton iterative solver which reduces the overall errors in the guess. Overall this implementation's computational cost is linear in terms of the number of reference points.
 
 Results of this method for our geometric reference point data set are shown in Table \ref{EPnP_Table}. Overall the performance of this calculation is robust when four non co-planar points are observed. A notable issue with this method is seen in cases where the points are co-planar. This is clearly shown in (Fig \ref{Fig:cone_performance}) for the case where the cone's base occludes the non co-planar reference points. The error from this image corresponds to the maximum error observed for the cases where occlusion is included. 
 
 Other than these outliers cases this method is great for cases where the reference points are well defined and non-coplanar. 
 
 It should be noted that this method also has a strict assumption that the reference points are on a rigid body. This is due to the fact that the 3D geometry of the reference points must be known beforehand to feed to the algorithms. . 
 

\begin{table}[h]
	\caption{EPnP Performance}
	\label{EPnP_Table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		
		\hline
		\multicolumn{5}{|c|}{Cone Error} \\
		\hline
		Points: & All & With Occlusion & 4 & 3\\
		\hline
		RMS & 9.4E-13  & 1.2 & 3.8E-1 & 1.6\\
		MAX & 6.5E-12 & 26.8 & 6.2 & 8.1\\
		MIN & 1.5E-14 & 1.0E-14 & 4.3E-15 & 4.9E-4\\
		\hline
		\multicolumn{5}{|c|}{Cube Error} \\
		\hline
		Points: & All & Non-Occluded & 4 & 3\\
		\hline
		RMS & 2.9E-13 & 1.9E-13 & 4.8E-1 & 1.4\\
		MAX & 3.2E-12 & 1.7E-12 & 7.7 & 7.9\\
		MIN & 4.9E-15 & 3.2E-15 & 2.6E-15 & 3.5E-3\\
		\hline
		\multicolumn{5}{|c|}{Sphere Error} \\
		\hline
		Points: & All & Non-Occluded & 4 & 3\\
		\hline
		RMS & 9.7E-13 & 5.7E-13 & 5.2E-1 & 1.5\\
		MAX & 9.1E-12 & 3.8E-12 & 7.3 & 8.1\\
		MIN & 6.0E-15 & 6.3E-15 & 2.0E-15 & 2.5E-3\\
		\hline
		
		
	\end{tabular}
\end{table}

\begin{figure}
	\hspace{-10 ex}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=1.9in]{poor_cone_camera_view_w_error}
		\caption{View from Camera Position}
		\label{fig:vive}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=1.9in]{poor_cone_topview_view_w_error}
		\caption{View From Top}
		\label{fig:oculus}
	\end{subfigure}
	\caption{Figure shows the reference points of the cone which correspond to the max error in the case with occlusion. Blue circles represent non-occluded reference points. Red circles represent reference points occluded from the camera. Yellow star represents the estimated pose. Note that all point estimations are about the same position due to the poor estimation capability for co-planar reference points.}
	\label{Fig:cone_performance}
\end{figure}


\section{SVM Regression}

\subsection{Implementation}
We implemented our own version of the regression support vector machine (SVM) in Matlab using it's quadratic programming routines. The regression SVM is formulated as the optimization of the $\epsilon$ - insensitive loss function:

\begin{gather}
min\; \frac{1}{2} || \omega ||^2 + C \sum_{i = 1}^{n} (\xi_i + \xi_i^*)\\
\nonumber y_i - f(x_i, \omega) \leq \epsilon + \xi_i^*\\
\nonumber f(x_i, \omega) - y_i \leq \epsilon + \xi_i\\
\nonumber \xi_i, \xi_i^* \geq 0 \; \; \forall \; \; i
\end{gather}

\noindent
Where $y$ is the vector of true values, $f(x, \omega)$ is the regression, $\xi$ and $\xi^*$ are slack variables, and $C$, and $\epsilon$ are hyper-parameters.

The Lagrangian dual formulation of this problem is

\begin{gather}
max -\frac{1}{2} \sum_{n=1}^N \sum_{m=1}^N ( \alpha_n - \alpha_n^*)
      (\alpha_m - \alpha_m^*) k(x_n, x_m)\\
      \nonumber - \epsilon \sum_{n=1}^N(\alpha_n + \alpha_n^*) + \sum_{n=1}^N (\alpha_n - \alpha_n^*) y_n\\
\nonumber 0 \leq \alpha_i , \alpha_i^* \leq C
\end{gather}


We used the quadprog routine of Matlab to optimize this problem. Matlab optimizes:

\begin{gather}
\overset{min}{x} \frac{1}{2}x^T H x + f^T\\
\nonumber A x \leq b\\
\nonumber A_{eq} x = b_{eq}\\
\nonumber b_{lower} \leq x \leq b_{upper}
\end{gather}

\noindent And we set:

\begin{gather}
H = 
\begin{bmatrix}
K & -K\\
-K & K
\end{bmatrix}\\
f =
\begin{bmatrix}
\epsilon - y\\
\epsilon + y
\end{bmatrix}\\
A_{eq} = 
\begin{bmatrix}
1 \\
\dots \\
-1 \\
\dots
\end{bmatrix}\\
b_{eq} = 0\\
b_{lower} = 0 \; \; b_{upper} = C
\end{gather}

\noindent Where $K$ is the Gram matrix.

We made the slightly surprising discovery that Matlab quadratic programming doesn't seem to to support optimizing over multiple dimensions at once. Thus, for our investigations into the regression SVM, we actually created 6 SVMS (X translation, Y translation, Z translation, Yaw, Pitch, Roll) for each of the 3 different test shapes.

We used a Gaussian Radial Basis kernel for both our geometric reference point and the image data set. The bitmaps were simply changed into a 64x64x3 length vector (which we normalized to $[-0.5, 0.5]$) and passed into the kernel as usual. For the $N$ points of each shape in the reference point dataset, we formed a vector of $2N$ dimensions. (The X, and Y coordinates of each 2D projected point.)  For example, we had 14 reference points in our cube poses, and thus formed each 2D projection into a length 28 vector.

This formulation on the geometric reference point set ignores the effects of occlusion. Hypothetically, to best match the problem, we should not provide information from points that are occluded. We did attempt to derive a kernel that could encode the knowledge of which points were occluded, define a kernel based on the GRBK which only considered points that were not occluded in both samples. (All other entries corresponded to points occluded in either sample were set to zero.)
Unfortunately, Matlab quickly informed us using this kernel made the problem non-convex, and we suspect this operation is not actually positive semi-definite.

We divided our 500 poses from each shape into a training set of 250 samples, a validation set of 125 samples, and a testing set of 125 samples. Our figure of merit was the root mean square value of the residuals between the real translation or rotation coordinates of our data, and those predicted by our SVM. (Recall, we trained a different SVM for each of the 6 dimensions of the pose.)

For both our datasets we have three hyper-parameters to optimize for: $C$, $\epsilon$, and $\sigma$ (the bandwidth of the kernel). We optimized our hyper-parameters by using an empirically based coordinate descent algorithm. We varied our hyper-parameters over:

\begin{align*}
C &\in [10^{6} ... 10^{100}]\\
\sigma &\in [10^{-2} ... 10^{2}]\\
\epsilon &\in [10^{-3} ... 10^{3}]
\end{align*}

\noindent where 100 test points were in each parameter sweep, spread evenly logarithmically.

In turn, we optimized each of the hyper-parameters keeping the others constant. We trained an SVM for each of the 100 possible values for the hyper-parameter in question, and then set it to the value that produced the lowest validation error. We then continued on with the next hyper-parameter, cycling through until validation error stayed constant through a cycle of all the hyper-parameters.

\subsection{Performance on Reference Point Data}

\begin{table}[h]
\caption{SVM Performance on the Reference Point Dataset}
\label{SVM_Point}
\centering
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	Shape & Dimension & Train E. & Val E. & Test E\\
   \hline
    Cone &X & 0.028562 & 0.097226 & 1.203973 \\
    Cone &Y & 0.017899 & 0.096197 & 1.324511 \\
    Cone &Z & 0.074570 & 0.240741 & 0.673912 \\
    Cone &Yaw & 65.554484 & 64.610879 & 143.902763 \\
    Cone &Pitch & 9.679785 & 19.332789 & 67.931383 \\
    Cone &Roll & 22.610235 & 28.204543 & 63.173690 \\
    \hline
    Cube & X & 0.006369 & 0.051618 & 1.246132 \\
    Cube & Y & 0.010817 & 0.038766 & 1.148181 \\
    Cube & Z & 0.050916 & 0.115780 & 0.719099 \\
    Cube & Yaw & 54.671545 & 69.127465 & 128.892843 \\
    Cube & Pitch & 11.516737 & 15.104284 & 70.884934 \\
    Cube & Roll & 22.665185 & 27.860259 & 64.804574 \\
    \hline
    Sphere & X & 0.031519 & 0.078171 & 1.208821 \\
    Sphere & Y & 0.019581 & 0.069591 & 1.313238 \\
    Sphere & Z & 0.085546 & 0.170599 & 0.715436\\
    Sphere & Yaw & 61.834910 & 76.257994 & 124.865239 \\
    Sphere & Pitch & 15.551304 & 16.981590 & 67.641860 \\
    Sphere & Roll & 15.323147 & 36.774045 & 62.371281 \\
    \hline
	\end{tabular}
\end{table}

Table \ref{SVM_Point} shows our RMS error for pose estimation using the geometric reference points (angles in degrees) with the optimized hyper-parameters we found by coordinate descent. The first thing we note, of course, is that these values are terrible! We appear to be doing better than random chance, but not by much. We see the largest error in Yaw, which we think might be due to the ambiguous discontinuity near 360 / 0 degrees. The other dimensions do not have a wrap around point as they were only varied 180 degrees.

We note a very interesting trend in the relative differences between train, test, and validation error. Generally, our training error was less than our validation error, suggesting over-fitting, but we also note many times the validation error was significantly less than the test error! The biggest examples of this effect can be seen in the translation dimensions. Our training and validation errors are small, on the order of $10^{-2}$ or $10^{-1}$, but the test error is over 1. We theorize that perhaps our 500 samples are too sparse in the dimensionality of the problem, such that learning results near the validation set, does not mean areas near the test set have been learned. We did check to see if there were some correlations in the test set to coverage in the randomly generated poses, but they do indeed appear random.

\begin{table}[h]
\caption{SVM Optimized Parameters on the Reference Point Dataset}
\label{SVM_Point_Parms}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dim & C & $\sigma$ & $\epsilon$\\
\hline
Cone & X & 65.793322 & 29.836472 & 0.012328\\
Cone & Y & 151.991108 & 35.938137 & 0.006136\\
Cone & Z & 132.194115 & 27.185882 & 0.043288\\
Cone & Yaw & 8697.490026 & 83.021757 & 4.328761\\
Cone & Pitch & 1629.750835 & 18.738174 & 2.848036\\
Cone & Roll & 10000.000000 & 100.000000 & 4.328761\\
\hline
Cube & X & 21.544347 & 4.641589 & 0.001748\\
Cube & Y & 26560.877829 & 62.802914 & 0.006136\\
Cube & Z & 65.793322 & 8.111308 & 0.018738\\
Cube & Yaw & 1873.817423 & 6.734151 & 0.001000\\
Cube & Pitch & 5722.367659 & 17.073526 & 3.764936\\
Cube & Roll & 2848.035868 & 14.174742 & 0.533670\\
\hline
Sphere &  X & 9.326033 & 15.556761 & 0.009326\\
Sphere &  Y & 70548.023107 & 100.000000 & 0.012328\\
Sphere &  Z & 1873.817423 & 100.000000 & 0.049770\\
Sphere &  Yaw & 20092.330026 & 100.000000 & 0.464159\\
Sphere &  Pitch & 3274.549163 & 52.140083 & 0.001000\\
Sphere &  Roll & 705.480231 & 8.111308 & 1.417474\\
\hline
\end{tabular}
\end{table}

Table \ref{SVM_Point_Parms} shows what our optimized hyper-parameters were. We note a few general trends. First, the angle dimensions have much larger epsilons, which intuitively makes sense as they span over larger order of magnitudes than the translation components. C was also, in general, higher for angle dimensions than translational, with the notable exceptions of the Y dimensions for both the sphere and the cube.

\subsection{Performance on Raw Image Data}

\begin{table}[h]
\caption{SVM Performance on the Image Dataset}
\label{SVM_Image}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dimension & Train E. & Val E. & Test E\\
\hline
Cone & X & 0.929743 & 0.810887 & 0.856581\\
Cone & Y & 0.984332 & 0.818735 & 0.886178\\
Cone & Z & 0.490626 & 0.496716 & 0.519886\\
Cone & Yaw & 108.586440 & 98.178370 & 102.914391\\
Cone & Pitch & 60.378916 & 51.779552 & 54.047725\\
Cone & Roll & 52.591000 & 50.459018 & 47.921814\\
\hline
Cube & X & 0.897220 & 0.884600 & 0.885430\\
Cube & Y & 0.883796 & 0.779065 & 0.925021\\
Cube & Z & 0.512376 & 0.520755 & 0.508106\\
Cube & Yaw & 99.024495 & 101.423926 & 98.324162\\
Cube & Pitch & 57.957975 & 48.915493 & 50.332767\\
Cube & Roll & 53.386639 & 50.835804 & 53.885835\\
\hline
Sphere & X & 0.879085 & 0.951192 & 0.906496\\
Sphere & Y & 0.958136 & 0.792612 & 0.912514\\
Sphere & Z & 0.474422 & 0.491515 & 0.509054\\
Sphere & Yaw & 104.293443 & 101.257011 & 100.387393\\
Sphere & Pitch & 58.114635 & 52.445369 & 57.654654\\
Sphere & Roll & 55.780784 & 53.296742 & 59.528568\\
\hline
\end{tabular}

\end{table}

Table \ref{SVM_Image} shows the performance of our SVM regression using the full bitmap data. We note that the test data performance is marginally better than our reference point data set, but is still far inferior to the performance of EPnP. It is notable, that the large differences between test error and validation error have disappeared Training error is also much more in line with validation and test error. In fact, often times the test data set out-performed the training data set.

\begin{table}[h]
\caption{SVM Optimized Parameters on Image Dataset}
\label{SVM_Image_Parms}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dim & C & $\sigma$ & $\epsilon$\\
\hline
Cone & X & 464158.883361 & 100.000000 & 0.100000\\
Cone & Y & 4641.588834 & 14.677993 & 0.001000\\
Cone & Z & 46415.888336 & 0.100000 & 0.001778\\
Cone & Yaw & 215.443469 & 0.010000 & 0.056234\\
Cone & Pitch & 21544.346900 & 0.146780 & 5.623413\\
Cone & Roll & 0.215443 & 100.000000 & 0.001000\\
\hline
Cube & X & 21544.346900 & 21.544347 & 0.031623\\
Cube & Y & 0.010000 & 0.010000 & 0.001000\\
Cube & Z & 46415.888336 & 0.010000 & 0.001000\\
Cube & Yaw & 21544.346900 & 0.068129 & 0.056234\\
Cube & Pitch & 464158.883361 & 68.129207 & 1.000000\\
Cube & Roll & 4641.588834 & 0.010000 & 0.316228\\
\hline
Sphere & X & 0.010000 & 0.010000 & 0.001000\\
Sphere & Y & 10000.000000 & 68.129207 & 0.003162\\
Sphere & Z & 4641.588834 & 0.068129 & 0.001000\\
Sphere & Yaw & 0.100000 & 0.010000 & 0.001000\\
Sphere & Pitch & 464158.883361 & 0.146780 & 0.003162\\
Sphere & Roll & 0.464159 & 0.010000 & 0.010000\\
\hline

\end{tabular}

\end{table}

Our optimized parameters from the image data set are shown in Table \ref{SVM_Image_Parms}.  We note again the higher values of $\epsilon$ showed up in the angle dimensions.

\subsection{Sparsity of SVM Solutions}
\begin{figure}[H]
	\hspace{-10 ex}

	\centering
	\includegraphics[width=0.9\linewidth]{supportVectors}

	\caption{The number of support vectors for each dimension in both the reference point and bitmap data sets. The image data set created a much sparser solution (in terms of numbers of support vectors) than the bitmap data set.}
	\label{supportVectors}
\end{figure}

Fig \ref{supportVectors} shows the number of support vectors generated in each of our SVMs. Generally, we found that the SVMs trained to work on full bitmaps generated a sparser solution than those SVMs trained on the geometric point data sets. In several cases, we see that there was no reduction in solution size at all. This suggests we are not sufficiently filling the input feature space with enough training examples; there's no "redundant" samples to ignore. That does raise the question of why the image SVMs made sparser solutions; they performed better but the input data was of a much higher dimensionality?


\section{Feed-Forward Network (FFNN)}
We implemented a feed forward neural network in MATLAB (based on our code from HW3). We used this network to train and predict on the geometric reference point data set. (The fully connected nature and the large data size of the bitmaps made running on a MATLAB based code too slow for practicality.)

We used ReLUs for our activation functions in the hidden layers. All of our weights were initialized to a normal random variable with variance $1/\sqrt{m}$ where $m$ is the number of neurons in the previous layer. Biases were initialized to 0. This initialization scheme means that the magnitude of the input to a neuron should not depend on how many neurons are in the previous layer. Our output layer was simply a linear output, as we were performing regression and not classification. We used squared loss as our loss function. Unlike the SVM, the neural network was able to output a complete prediction for all dimensions of the pose, as opposed to our SVM case where we had different regressions for each dimension.

Instead of outputting a 6 dimensional pose, we opted to convert the poses to a 3 dimensional translation concatenated to a 9 dimensional direction cosine matrix. The direction cosine matrix (DCM) is a 3x3 matrix where each column is an axis in the body frame of the shape, with the rotation of the pose applied.

\begin{gather}
\nonumber \hat x = R \cdot \vec x \; \; \; \; \hat y = R \cdot \vec y \; \; \; \; \hat z = R \cdot \vec z\\
DCM =
\begin{bmatrix}
\hat x & \hat y & \hat z
\end{bmatrix}
\end{gather} 

\noindent Where $\vec x$, $\vec y$, and $\vec z$ are the unit vectors for the coordinate system, and $R$ is the rotation matrix of the pose.

There are two benefits of this system. The first is that there are no ambiguities with wrap-around (like the with the yaw coordinate that we saw with the SVMs). The second is that now the dimensions of the prediction are all approximately around the same order of magnitude. Previously, angular components varied all the way to 360, but now, all components are approximately around magnitude 1. This will help prevent the neural network from favoring one coordinate over the other. We, of course, also converted our true pose parameters into this 12-vector form as well.

Like the SVMs, we used a coordinate descent empirical algorithm to search for an optimum configuration. For each training, we trained with a batch size of 1, and terminated when the validation error (the RMS of the norms between the predicted 12-vectors and the true 12-vectors) did not improve for 10 epochs (an epoch being presenting 250 randomly drawn training samples). We varied 6 hyper-parameters, the number of neurons in 5 layers ( $L_1 \dots L_5$ ), and the learning rate $\eta$.

\begin{gather}
\eta \in \{  10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-8} \}\\
L_i \in \{  0, 2, 8, 32, 128, 256  \}
\end{gather}

\noindent (An $L_i$ of 0 indicated we just dropped the layer, so we varied different total number of layers in the architechture. As there is some randomness in the final validation accuracy due to initialization, we couldn't rely on a monotonically decreasing error and it's eventually shallowing out to determine when to terminate. We thus used a termination of 10 full cycles of optimizing every hyper-parameter.

\begin{table}
\caption{Optimal hyper-parameters for Feed-Forward Neural Networks}
\label{FFNN_Params}
\centering

\begin{tabular}{|c|c|c|}
\hline
Shape & $L$ & $\eta$\\
\hline
Cube & [32, 256, 256 ] & .01\\
Cone & [128, 128 ] & .01\\
Sphere & [128, 128 ] & 0.01\\
\hline
\end{tabular}

\end{table}

\begin{table}
\caption{Feed-Forward Neural Network Performance}
\label{FFNN_Perf}
\centering

\begin{tabular}{|c|c|c|c|c|c|}
\hline
Shape & Train Error & Val Error & Test Error & Test Translational Error & Test Angular Error\\
\hline
Cube &0.0019    &0.1222   & 0.1462 & .2307 &  6.3224\\
Cone &0.0110   &0.1645    &0.1566 & .2091 & 6.5724\\
Sphere &0.0164    &0.1347   & 0.1696 & .2053 & 7.3250\\
\hline
\end{tabular}

\end{table}

Table \ref{FFNN_Params} shows the optimized hyper-parameters that our coordinate descent algorithm found for each shape. Two and three layer networks were the best, with large amount of neurons in. $\eta$ seems to have converged on 0.01 as the best step size.

Table \ref{FFNN_Perf} shows the performance of our optimal neural networks on the point data set. Validation and test error are the RMS values of the norm of the vector residual between the real 12-vector and the predicted 12-vector. To get a more intuitive measure of how the regression was performing, we also for the test set, listed the average translational error (norm of the residual between translation vectors) and the average angle between the body axiis of the predicted rotation versus the real rotation (in degrees).

We note that first, we have performed much better than the SVM. Translational error is smaller, and the average angular error has sunk to less than 10 degrees. This is still no where close to the accuracy, of the EPnP algorithm, but is much closer.

We note that based on the training error vs. the validation error and the test error, that we are overfitting. There are a number of ways we could have tried to remove this over-fitting if we had more time to study the problem, including noising up the point locations, or implementing drop-out. It's also possible, that we simply need to provide more samples during the training.

\subsection{Implementation}

\subsection{Performance on Reference Point Data}

\subsection{Performance on Raw Image Data}

\section{Convolutional Neural Network (CNN)}
We explored using a convolutional neural network on our image data only. This is because CNNs have the ability to relate features locality which could be beneficial for image data. we again note that due to the low dimensionality of the reference point data set, we did not test the CNN's performance with that data. 
\subsection{Implementation}
We implemented our CNN using a GPU version of TensorFlow. Similar to homework 3, we had an input layer, multiple convolution and pooling layers, multiple hidden layers, and an output layer.(Fig \ref{Fig:CNN_ARCH})

\begin{figure}
	\centering\hspace{-10 ex}
	\includegraphics[width=4in]{CNN_ARCH}
	\caption{Graphical Representation of our CNN Setup}
	\label{Fig:CNN_ARCH}
\end{figure}

For the input layer we took each channel(RGB) of the image and normalized the pixel values such that they could only lay within -0.5 to 0.5. The size of our input layer corresponded to the number of pixels times the number of channels. In our cases, all of our runs used a 64x64 pixel image with three channels that corresponded to red, blue, and green. For the output layer, we used 12 neurons; three neurons that represented the 3D position of the object and nine neurons that represented the objects orientation as a direction cosine matrix. We applied a linear output layer since we were attempting to perform regression and not classification. 

The number of layers, nodes, filter size and stride was tunable hyperparameters for the convolution, pooling layers. For the standard hidden layers, the number of layers and nodes were a tunable parameter. The convolutional and hidden layer both used Relu activation functions, while the pooling layers used maximum operation as activation functions. 



\subsection{Training}
For training the networks, we used a standard L2 Loss function as well the same gradient descent method used in homework 3. 

Since we were not totally sure the best hyperparameters to use for the data set, we decided to implement version of coordinate descent to attempt to maximize performance. 
In each iteration of coordinate descent we ran through the following list of hyperparameters:


\begin{align*}
Train.\ Batch\ Size  &\in [100]\\
Number\ of\ Training\ Steps  &\in [1000]\\
Learning\ Rate &\in [10^{-2},\ 10^{-4},\ 10^{-8}]\\
Conv.\ Filter\ Size &\in [3,\ 5,\ 8]\\
Conv.\ Stride &\in [1,\ 2,\ 4]\\
Conv.\ Depth &\in [8,\ 16]\\
Conv.\ Layer\ Pooling\  &\in [ON,\ OFF]\\
Pooling\ Filter\ Size &\in [3,\ 5,\ 8]\\
Pooling\ Stride &\in [1,\ 2,\ 4]\\
Weight\ Penalty\ Filter\ Size &\in [3,\ 5,\ 8]\\
Num\ Hidden\ Layer\ Neurons &\in [16,\ 32,\ 64,\ 128]\\
\end{align*}

We originally were considering including the number of convolution, pooling, and hidden layers as part of the hyper parameters sweep that we performed coordinate descent on. We were concerned that in doing so, the coordinate descent may not be correctly choosing the number of layers due to  of hyper parameters dependencies on those layers may causing a local minimum. We thought this may have a high likelihood of prematurely forcing the CNN layer architecture in to a local minimum but not a global minimum. We addressed this possible problem by choosing a set of layer architectures to run the coordinate descent with and did a comparison between them. We ran our coordinate descent for 10 iterations on the permutations of the following sets:

\begin{align*}
Num.\ Hidden Layers\  &\in [1,\ 2]\\
Num.\ Conv./Pooling\ Layers &\in [0,\ 1,\ 2]\\
\end{align*}


\subsection{Performance on Raw Image Data}



Performance on the raw image data sets In table /ref table you can see the performance over each iteration of the coordinate descent. Overall the performance of the CNN did not fair well for the image data. It is clear that in some cases coordinate descent improved over each iteration while others it seemed to have performance that bounced around. We decided to take the hyper parameters of the best performing iteration of each layer architecture and run those cases for a larger number of steps to see if the performance improved. Those results are shown in table /ref table. *Some intelligent statement of about that*. 

\begin{table}
	\caption{Optimal Architecture for Convolutional Neural Network}
	\label{CNN_Params}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Shape & Train RMS Error & Val RMS Error & Num Hidden & Num Conv. \\
	\hline
	Cube & 4.5070 & 4.6018 & 2 & 1  \\
	Cone & 4.4347 & 4.0840 & 1 & 2\\
	Sphere &4.5627 & 4.5696  & 2 & 0\\
	\hline
\end{tabular}

\end{table}


\begin{table}
	\caption{Optimal Hyperparameters for Convolutional Neural Network}
	\label{CNN_Params}
	\begin{tabular}{|r|c|c|c|}
		\hline
     	&\multicolumn{1}{|c|}{Cube} & \multicolumn{1}{|c|}{Cone} &
     	\multicolumn{1}{|c|}{Sphere}\\
     	\hline
     	Weight Penalty  & 0.1 & 0 &0\\
     	\hline
     	Learning Rate  & 10^{-4} & 10^{-4} & 10^{-4}\\
     	\hline
     	Neurons in Hidden Layer 1  & 16 & 16  &32\\
		\hline
		Neurons in Hidden Layer 2  & 32 & * &16\\
		\hline
		Conv. Layer 1 Filter Size  & 3 & 8 &*\\
		\hline
		Conv. Layer 2 Filter Size  & * & 3 &*\\
		\hline
		Conv. Layer 1 Stride  & 4 & 2 &*\\
		\hline
		Conv. Layer 2 Stride  & * & 2 &*\\
		\hline
		Conv. Layer 1 Depth  & 8 & 8 &*\\
		\hline
		Conv. Layer 2 Depth  & * & 8 &*\\
		\hline
		Pooling  & ON & ON & *\\
		\hline
		Pooling Layer 1 Filter Size  & 8 & 5 &*\\
		\hline
		Pooling Layer 2 Filter Size  & * & 3 &*\\
		\hline
		Pooling Layer 1 Stride & 41 & 4 &*\\
		\hline
		Pooling Layer 2 Stride  & * & 1 &*\\
		

		\hline
	\end{tabular}
	
\end{table}


To get a better understanding of the errors, we looked at the each pose component separately to see if we. Since the orientation of the pose is in DCM, we looked at the dot products of the estimated  and true normalized vectors of the x, y, and z components of the DCM to give us an idea of our accuracy. It should be noted that if the dot produce is equal to 1 then it means they matched perfectly and a dot produce of -1 means they are anti-parallel. On average we can see that  .....

\begin{figure}
	\hspace{-10 ex}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=1.9in]{train_coord_descent_plot}
		\caption{Training data RMS}
		\label{fig:train_coord_desc}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=1.9in]{val_coord_descent_plot}
		\caption{Validation data RMS}
		\label{fig:val_coord_desc}
	\end{subfigure}
	\caption{Figure shows RMS of the validation and training data after each coordinate descent iteration for each shape.}
	\label{Fig:cone_performance}
\end{figure}


\section{Summary}

\section{Division of Labor}

\begin{table}[h]
	\caption{Project Division Of Labor}
	\label{SomeTable}
	\centering
	\begin{tabular}{|c|c|}
		\hline
		Task &  Person\\
		\hline
		Data Set Generation &  Ben\\
		
		EPnP Baseline Work &  Alex\\
		SVM Regression & Ben\\
		FFNN & Ben \\
		CNN & Alex\\
		Report Writing & Alex and Ben\\
		\hline
		
		
	\end{tabular}
\end{table}



\begin{thebibliography}{10}
	
	\bibitem{Robust_PNP} 
	S. Li, C. Xu and M. Xie. \textit{A Robust O(n) Solution to the Perspective-n-Point Problem}, in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, p. 1444-1450, 2012.
	
	\bibitem{EPnP} 
	V. Lepetit, F. Moreno-Noguer and P. Fua. \textit{EPnP: An Accurate O(n) Solution to the PnP Problem}, in International Journal Of Computer Vision, vol. 81, p. 155-166, 2009.

	\bibitem{opengl_image}
	Image Source: \\\texttt{http://www.songho.ca/opengl/files/g\_projectionmatrix01.png}
	
		\bibitem{vive_image} 
	Image Source: \\\texttt{https://i.ytimg.com/vi/J54dotTt7k0/maxresdefault.jpg}
	
	
	\bibitem{oculus_image} 
	Image Source: \\\texttt{http://www.roadtovr.com/wp-content/uploads/2014/01/oculus-rift-crystal-cove.jpeg}

\end{thebibliography}

% For peer review papers, you can put extra information on the cover
% page as needed:
 \ifCLASSOPTIONpeerreview
 \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
 \fi



% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




   
\end{document}


%Example figure syntax

%\begin{figure}[h]
%\centering
%  \noindent
%  \centering{\hspace{-8 ex} \includegraphics[width=3in]{layers2}}
%  \caption{Boreholes and soil profile with measurements depths.}\label{Fig1Label}

%\end{figure}


%Example equation syntax
%\begin{equation}\label{eq:wavenum}
%k(z) = 2\pi f\sqrt{\epsilon_c(z)\mu} = \sqrt{(k^{mode_n}_\rho)^2 + %(k^{mode_n}_z)^2(z)},
%\end{equation}





