
\documentclass[journal]{IEEEtran}

% *** GRAPHICS RELATED PACKAGES ***
%

  %\usepackage[pdftex]{graphicx}
  \usepackage{graphicx}
  \usepackage{caption}
  \usepackage{subcaption}
  \usepackage{amsmath}
  % declare the path(s) where your graphic files are
 \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\usepackage[export]{adjustbox}
\usepackage{float}

\usepackage[section]{placeins}

% correct bad hyphenation here
\usepackage{fixltx2e}
 \usepackage{cite} 
\usepackage{url}


\usepackage{epstopdf}

\epstopdfDeclareGraphicsRule{.gif}{png}{.png}{convert gif:#1 png:\OutputFile}
\AppendGraphicsExtensions{.gif}

\setcounter{totalnumber}{5}
\setcounter{topnumber}{5}
\setcounter{bottomnumber}{5}
\renewcommand{\topfraction}{1}
\renewcommand{\bottomfraction}{1}

\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
% Do not put math or special symbols in the title.
\title{Project: 3D Pose Estimation from 2D Projections}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Richard (Alex) Showalter-Bucher\\Benjamin Yu}


\maketitle

\section{Introduction}
In this project, we explored the application of machine learning to the problem of estimating an object's 3D position and orientation, known as a pose, from a 2D projection. Being able to accurately perform this estimation has applications in several disciplines including Computer Vision, Photogrammetry, and Robotics\cite{Robust_PNP}. A notable application is object tracking in virtual reality systems (VR), such as the HTC Vive and Oculus Rift. Both of these systems estimate 2D angular locations of reference points on the objects being tracked to determine the object's pose.

For a system like the HTC Vive, this measurement is done directly via time of arrival of laser sweeps on reference points which are photodiodes. (Fig \ref{fig:vive}) In the case of the Oculus Rift, a camera is used to identify predetermined reference points (IR LEDs) before estimating the 2D angular location. This is done by blinking each LED in a specific pattern which is observed by a camera. (Fig \ref{fig:oculus}) After the reference points are identified they are centroided to determine their angular location on the focal plane. Once the 2D angular locations are estimated a Perspective N-Point method is applied to estimate the objects pose. In the next subsection we discuss what this method in more detail. 


\begin{figure}
	\hspace{-10 ex}
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{vive}
		\caption{HTC Vive\cite{vive_image}}
		\label{fig:vive}
	\end{subfigure}%
	\begin{subfigure}{.3\textwidth}
		\centering
		\includegraphics[width=.9\linewidth]{oculus}
		\caption{Oculus Rift\cite{oculus_image}}
		\label{fig:oculus}
	\end{subfigure}
	\caption{Left: HTC Vive's laser sweep with reference points. Right: Oculus Rift's camera with reference points}
	\label{fig:Vive and Oculus}
\end{figure}


\subsection{Perspective N-Point (PnP) Method}
The problem of estimating an objects 3D pose from 2D projections of known reference points is referred to as the "Perspective N-Point" problem. (PnP) 
Common approaches to this problem involve directly or iteratively solving a system of equations\cite{EPnP}. While these approaches can be very precise with the well defined reference points (Minimum of 4 non co-planar points for a non-ambiguous solution), there are usually trade-offs between accuracy and compute time. Direct solvers of these system of equations can be performed computationally fast, but may not be robustly accurate. On the other hand, iterative methods tend to be slower but more robust. A well-formed hybrid of these solvers can be both fast and robustly accurate for well defined points. One of these methods was used to baseline our performance using machine learning algorithms. That method is discussed in more detail in the Baseline Performance EPnP section.



\subsection{Motivation for Applying Machine Learning}
We wanted to implement and test several machine learning techniques to see how they fair with handling the pose estimation problem. There were three different types of techniques we used: SVM Regression, Feed-Forward Neural Networks (FFNN), and Convolutional Neural Networks(CNN). 

We used two different datasets to train our models: raw RGB image data and 2D projections of geometric reference points from an object.
These data types are similar to the raw and processed data, respectively, of the Oculus Rift system. 

Using the reference point data, we were curious if we could match the performance of the PnP method and, if we could obtain similar performance using many points, determine if pose regression using machine learning was robust using less than 4 points (the limitation for non-ambiguous solutions using PnP). Due to the sparsity of this type of data, set we did not train a CNN with it; we tested this type of data only on the SVM and FFNN. 

For the raw image data, we were curious if we could train our models such that they could directly estimate the poses of objects without the need to perform multiple processing steps such as centroiding. If this method worked robustly, then tracked objects may be able to remove explicit defined reference points which would allow for a more cost effective and general solution to the pose estimation problem. 
 
 We also note the use of PnP algorithms require that the reference points, no matter how their locations are measured, are only useful if their exact relative geometry to one another. That is, the PnP algorithms are given knowledge of what the object looks like. In the case our machine learning algorithms, the geometry of the objects are not specified, and must be learned during the training phase. The challenge of tracking objects would then be free of the problem of getting exact geometric measurements.
 
Unfortunately, we found that none of our techniques met the performance of the PnP.  We were hoping to get comparable results to PnP methods but found that the estimations from all data sets were significantly worse than the PnP. We discuss this in more detail in each techniques respective section.

In the next section we discuss our data set and how it was generated. 


\section{Data Set Generation}

To test our machine learning and baseline PnP algorithm we generated two data sets. Each dataset contains versions of a cube, a cone, and a sphere in 500 randomly generated poses. (So each dataset contains 1500 total different samples.) Each pose contains a 3D translation and then yaw, pitch, and roll rotations.

\subsection{Geometric Data Points}

The first data set consists of the projected 2D locations of geometric points from the various shapes. We defined a set of 3D points on the surfaces of the cube, the cone, and the sphere. These base shapes were centered around the origin of our coordinate system. Then, a randomly generated translation and rotation (a pose) was applied to the points. Finally, the points were projected onto a 2D image using a sixty degree field of view.

The geometry of the world was set in a right-handed coordinate system. The camera is at the origin and points along the negative z-axis, so that the x-axis points to the right, and the the y-axis points up (from the point of view of the camera).

The base geometry of each shape was chosen to to have a characteristic dimension of 1.0. The sphere had a diameter of 1.0. The cone had a diameter of 1.0 at its base, and a height of 1.0. The cube had sides of length 1.0. The rotations were allowed to vary uniformly through the space. Yaw varied between 0 and 360 degrees. Pitch varied between positive 90 and negative 90 degrees. Roll varied between 0 and 180 degrees. Those spans are enough to specify any 3D rotation. The x and y of the coordinates of the translation were uniformly distributed between -1.5 and +1.5. The z coordinated varied between -4.33 and -1.7320. This z range placed our shapes at a distance from the camera so that they took up roughly one fifth to a third of the total FOV. We note it was possible for a shape to be half off the image.

The math for applying our random pose, and then projecting our points onto a 2D image follow the vertex processing of OpenGL 1.0. We start with our points from the base shape. These are converted to homogeneous coordinates, which are 4-vectors of the form:

**EQUATION FOR 3D POINT IN HOMOGENOUS COORDINATES**

The benefit of this coordinate system is that both translations and rotations can be written as a matrix multiplication. We create one matrix for the translation, as well as for each of the yaw, pitch, and roll rotations. Finally, we calculate a projection matrix which defines a frustum that represents the field of view of the camera.

**Figure of frustrum**

We then calculate the final locations of the points:

**Equation of rotation**

We normalize these coordinates, by dividing each element of the 4th component of each vector. These normalized coordinates have the property that every point inside that frustum has x, y, and z coordinates in [-1, 1]. Anything else is out of the FOV.

We also calculated the occlusion of each of these points. The final points have z-coordinates, and by defining a set of triangles on the surface of each shape, it is possible to see if a point is blocked by some surface segment of the shape. In the end we record only the location of the projected points on the x-y plane and if the point was occluded.

**FIGURES OF SHAPES**

\subsection{Image Data Set}
Our second data set consisted of full bitmap images of posed cubes, cones, and spheres on a black background. We used the GLUT library to speed our creation of these scenes with OpenGL. Like before, we created 500 random poses of each of the three shapes. The math implemented is the same as in the geometric point case, but of course, OpenGL will render a full scene after transforming the shape vertices.

We applied a texture to our shapes. We chose a high-res version of the Earth from NASA public release. We figured this texture was interesting and also asymmetric. Lack of symmetry is beneficial, as for example, it's possible to, for example, determine which face of the cube you are observing; each face will have a unique part of the Earth on it.

We rendered 64x64 24-bit RGB images and saved them in BMP format. The images are saved unnormalized, but we applied normalization in our algorithms.

**FIGURES OF IMAGE SHAPES**

\section{Baseline Performance of EPnP}

 As a baseline to the performance of the 3D estimation, we used a method referred to as Efficient PnP (EPnP). The implementation of the solver used was included as part of a paper by Lepetit et al\cite{EPnP}. This method is a hybrid method between a direct and iterative solver. In first part of the algorithm, a weighted sum of four virtual points are calculated from n observed reference points. These four virtual points are then used in the direct solver which gives an initial guess of the solution. This guess is then fed into a Gauss-Newton iterative solver which reduces the overall errors in the guess. Overall this implementation's computational cost is linear in terms of the number of reference points.
 
 Results of this method for our geometric reference point data set are shown in Table \ref{EPnP_Table}. Overall the performance of this calculation is robust to some level of noise as long as the points are non-coplanar. A notable issue with this method is seen in cases where the points are co-planar. For example, if the reference points on the base of the cone are the only reference points visible then the direct solver has an issue getting a reasonable initial guess to feed the Gauss-Newton solver.
 
 **(Show figure of examples?)**
 
 This method is great for cases where there the reference points are well defined and non-coplanar. It should be noted that this method also has a strict assumption that the reference points are on a rigid body. This is due to the fact that the 3D geometry of the reference points must be known beforehand to feed to the algorithms. 
 
 

\begin{table}[h]
	\caption{EPnP Performance: Measure Errors = 0, 0.05, 0.1 degs }
	\label{EPnP_Table}
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		
		\hline
		\multicolumn{5}{|c|}{Cone Error} \\
		\hline
		Points: & All & Non-Occluded & 4 & 3\\
		\hline
		RMS & \multicolumn{1}{|l|}{9E-13}  &  &  &\\
		MAX & 6E-12| & &  &\\
		MIN & 2E-14| & &  &\\
		\hline
		\multicolumn{5}{|c|}{Cube Error} \\
		\hline
		Points: & All & Non-Occluded & 4 & 3\\
		\hline
		RMS & 2E-13| &  &  &\\
		MAX & 3E-12 & &  &\\
		MIN & 4E-15 & &  &\\
		\hline
		\multicolumn{5}{|c|}{Sphere Error} \\
		\hline
		Points: & All & Non-Occluded & 4 & 3\\
		\hline
		RMS &  &  & &\\
		MAX &  & &  &\\
		MIN & & &  &\\
		\hline
		
		
	\end{tabular}
\end{table}

\section{SVM Regression}

\subsection{Implementation}
We implemented our own version of the regression support vector machine SVM) in Matlab using it's quadratic programming routines. The regression SVM is formulated as the optimization of the $\epsilon$ - insensitive loss function:

** EQUATION FOR E INSENSITIVE LOSS **

The Lagrangian dual formulation of this problem is

**DUAL FORM OF SVM EQUATION **

We used the quadprog routine of Matlab to optimize this problem. Specifically, we set:

**MATLAB VARIABLES HERE EQUATION**

We made the slightly surprising discovery that Matlab quadratic programming doesn't seem to to support optimizing over multiple dimensions at once. Thus, for our investigations into the regression SVM, we actually created 6 SVMS (X translation, Y translation, Z translation, Yaw, Pitch, Roll) for each of the 3 different test shapes.

We used a Gaussian Radial Basis kernel for both our geometric reference point and the image data set. The bitmaps were simply changed into a 64x64x3 length vector and passed into the kernel as usual. For the $N$ points for each shape in the reference point dataset, we formed a vector of $2N$ dimensions. (The X, and Y coordinates of each 2D projected points.)  For example, we had 14 reference points in our cube poses, and thus formed each pose into a length 28 vector.

This formulation on the geometric reference point set ignores the effects of occlusion. Hypothetically, to best match the problem, we should not provide information from points that are occluded. We attempted to derive a kernel that could encode the knowledge of which points were occluded, define a kernel based on the GRBK as:

**MODIFIED KERNEL FORMULATION**

Unfortunately, Matlab quickly informed us using this kernel made the problem non-convex, and we suspect this operation is not actually positive semi-definite.

For both our datasets we have 3 hyper-parameters to optimize for: $C$, $\epsilon$, and $\sigma$ (the bandwidth of the kernel). We optimized our hyper-parameters by using an empirically based coordinate descent algorithm. We varied our hyper-parameters over:

\begin{align*}
C &\in [10^{6} ... 10^{100}]\\
\sigma &\in [10^{-2} ... 10^{2}]\\
\epsilon &\in [10^{-3} ... 10^{3}]
\end{align*}

\noindent 100 test points were in each parameter sweep, spread evenly logarithmically.

We divided our 500 poses from each shape into a training set of 250 samples, a validation set of 125 samples, and a testing set of 125 samples. Our figure of merit was the root mean square value of the residuals from the actual values of the translation X, Y, Z, and rotations yaw, pitch, roll. (Recall, we trained a different SVM for each of the 6 dimensions of the pose.) In turn, we optimized each of the hyper-parameters keeping the others constant. We trained an SVM for each of the 100 possible values for the hyper-parameter in question, and then set it to the value that produced the lowest validation error. We then continued on with the next hyper-parameter, cycling through until validation error stayed constant through a cycle of all the hyper-parameters.

\subsection{Performance on Reference Point Data}

\begin{table}[h]
\caption{SVM Performance on the Reference Point Dataset}
\label{SVM_Point}
\centering
	\begin{tabular}{|c|c|c|c|c|}
	\hline
	Shape & Dimension & Train E. & Val E. & Test E\\
   \hline
    Cone &Yaw & 65.554484 & 64.610879 & 143.902763 \\
    Cone &Pitch & 9.679785 & 19.332789 & 67.931383 \\
    Cone &Roll & 22.610235 & 28.204543 & 63.173690 \\
    Cone &X & 0.028562 & 0.097226 & 1.203973 \\
    Cone &Y & 0.017899 & 0.096197 & 1.324511 \\
    Cone &Z & 0.074570 & 0.240741 & 0.673912 \\
    \hline
    Cube & Yaw & 54.671545 & 69.127465 & 128.892843 \\
    Cube & Pitch & 11.516737 & 15.104284 & 70.884934 \\
    Cube & Roll & 22.665185 & 27.860259 & 64.804574 \\
    Cube & X & 0.006369 & 0.051618 & 1.246132 \\
    Cube & Y & 0.010817 & 0.038766 & 1.148181 \\
    Cube & Z & 0.050916 & 0.115780 & 0.719099 \\
    \hline
    Sphere & Yaw & 61.834910 & 76.257994 & 124.865239 \\
    Sphere & Pitch & 15.551304 & 16.981590 & 67.641860 \\
    Sphere & Roll & 15.323147 & 36.774045 & 62.371281 \\
    Sphere & X & 0.031519 & 0.078171 & 1.208821 \\
    Sphere & Y & 0.019581 & 0.069591 & 1.313238 \\
    Sphere & Z & 0.085546 & 0.170599 & 0.715436\\
    \hline
	\end{tabular}
\end{table}

Table \ref{SVM_Point} shows our RMS error for pose estimation using the geometric reference points (angles in degrees). The first thing we note, of course, is that these values are terrible! We appear to be doing better than random chance, but not by much. We see the largest error in Yaw, which we think might be due to the ambiguous discontinuity near 360 / 0 degrees. The other dimensions do not have a wrap around point as they were only varied 180 degrees.

We note a very interesting trend in the relative differences between train, test, and validation error. Generally, our training error was less than our validation error, suggesting overfitting, but we also note many times the validation error was significantly less than the test error! The biggest examples of this effect can be seen in the translation dimensions. Our training and validation errors are small, on the order of $10^{-2}$ or $10^{-1}$, but the test error is over 1. We theorize that perhaps our 500 samples are too sparse in the dimensionality of the problem, such that learning results near the validation set, does not mean areas near the test set have been learned. We did check to see if there were some correlations in the test set to coverage in the randomly generated poses, but they do indeed appear random.

\begin{table}[h]
\caption{SVM Optimized Parameters on the Reference Point Dataset}
\label{SVM_Point_Parms}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dim & C & $\sigma$ & $\epsilon$\\
\hline
Cone & Yaw & 8697.490026 & 83.021757 & 4.328761\\
Cone & Pitch & 1629.750835 & 18.738174 & 2.848036\\
Cone & Roll & 10000.000000 & 100.000000 & 4.328761\\
Cone & X & 65.793322 & 29.836472 & 0.012328\\
Cone & Y & 151.991108 & 35.938137 & 0.006136\\
Cone & Z & 132.194115 & 27.185882 & 0.043288\\
\hline
Cube & Yaw & 1873.817423 & 6.734151 & 0.001000\\
Cube & Pitch & 5722.367659 & 17.073526 & 3.764936\\
Cube & Roll & 2848.035868 & 14.174742 & 0.533670\\
Cube & X & 21.544347 & 4.641589 & 0.001748\\
Cube & Y & 26560.877829 & 62.802914 & 0.006136\\
Cube & Z & 65.793322 & 8.111308 & 0.018738\\
\hline
Sphere &  Yaw & 20092.330026 & 100.000000 & 0.464159\\
Sphere &  Pitch & 3274.549163 & 52.140083 & 0.001000\\
Sphere &  Roll & 705.480231 & 8.111308 & 1.417474\\
Sphere &  X & 9.326033 & 15.556761 & 0.009326\\
Sphere &  Y & 70548.023107 & 100.000000 & 0.012328\\
Sphere &  Z & 1873.817423 & 100.000000 & 0.049770\\
\hline
\end{tabular}
\end{table}

Table \ref{SVM_Point_Parms} shows what our optimized hyper-parameters were. We note a few general trends. First, the angle dimensions have much larger epsilons, which inutuitively makes sense as they span over larger order of magnitudes than the translation components. C was also, in general, higher for angle dimensions than translational, with the notable exceptions of the Y dimensions for both the sphere and the cube.

\subsection{Performance on Raw Image Data}

\begin{table}[h]
\caption{SVM Performance on the Image Dataset}
\label{SVM_Image}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dimension & Train E. & Val E. & Test E\\
\hline
Cone & X & 0.929743 & 0.810887 & 0.856581\\
Cone & Y & 0.984332 & 0.818735 & 0.886178\\
Cone & Z & 0.490626 & 0.496716 & 0.519886\\
Cone & Yaw & 108.586440 & 98.178370 & 102.914391\\
Cone & Pitch & 60.378916 & 51.779552 & 54.047725\\
Cone & Roll & 52.591000 & 50.459018 & 47.921814\\
\hline
Cube & X & 0.897220 & 0.884600 & 0.885430\\
Cube & Y & 0.883796 & 0.779065 & 0.925021\\
Cube & Z & 0.512376 & 0.520755 & 0.508106\\
Cube & Yaw & 99.024495 & 101.423926 & 98.324162\\
Cube & Pitch & 57.957975 & 48.915493 & 50.332767\\
Cube & Roll & 53.386639 & 50.835804 & 53.885835\\
\hline
Sphere & X & 0.879085 & 0.951192 & 0.906496\\
Sphere & Y & 0.958136 & 0.792612 & 0.912514\\
Sphere & Z & 0.474422 & 0.491515 & 0.509054\\
Sphere & Yaw & 104.293443 & 101.257011 & 100.387393\\
Sphere & Pitch & 58.114635 & 52.445369 & 57.654654\\
Sphere & Roll & 55.780784 & 53.296742 & 59.528568\\
\hline
\end{tabular}

\end{table}

Table \ref{SVM_Image} shows the performance of our SVM regression using the full bitmap data. We note that the performance is marginally better than our reference point data set, but is still far inferior to the performance of EPnP. It is noteable, that the large differences between test error and validation error have dissappeared. Training error is also much more in line with validation and test error. In fact, often times the test data set out-performed the training data set.

\begin{table}[h]
\caption{SVM Optimized Parameters on Image Dataset}
\label{SVM_Image_Parms}
\centering

\begin{tabular}{|c|c|c|c|c|}
\hline
Shape & Dim & C & $\sigma$ & $\epsilon$\\
\hline
Cone & X & 464158.883361 & 100.000000 & 0.100000\\
Cone & Y & 4641.588834 & 14.677993 & 0.001000\\
Cone & Z & 46415.888336 & 0.100000 & 0.001778\\
Cone & Yaw & 215.443469 & 0.010000 & 0.056234\\
Cone & Pitch & 21544.346900 & 0.146780 & 5.623413\\
Cone & Roll & 0.215443 & 100.000000 & 0.001000\\
\hline
Cube & X & 21544.346900 & 21.544347 & 0.031623\\
Cube & Y & 0.010000 & 0.010000 & 0.001000\\
Cube & Z & 46415.888336 & 0.010000 & 0.001000\\
Cube & Yaw & 21544.346900 & 0.068129 & 0.056234\\
Cube & Pitch & 464158.883361 & 68.129207 & 1.000000\\
Cube & Roll & 4641.588834 & 0.010000 & 0.316228\\
\hline
Sphere & X & 0.010000 & 0.010000 & 0.001000\\
Sphere & Y & 10000.000000 & 68.129207 & 0.003162\\
Sphere & Z & 4641.588834 & 0.068129 & 0.001000\\
Sphere & Yaw & 0.100000 & 0.010000 & 0.001000\\
Sphere & Pitch & 464158.883361 & 0.146780 & 0.003162\\
Sphere & Roll & 0.464159 & 0.010000 & 0.010000\\
\hline

\end{tabular}

\end{table}

Our optimized parameters from the image data set are shown in Table \ref{SVM_Image_Parms}.  We note again the higher values of $\epsilon$ showed up in the angle dimensions.

**STUFF ABOUT SPARSENESS AND NUMBER OF SUPPORT VECTORS**

\section{Feed-Forward Network (FFNN)}

\subsection{Implementation}

\subsection{Performance on Reference Point Data}

\subsection{Performance on Raw Image Data}

\section{Convolutional Neural Network (CNN)}
We explored using a convolutional neural network on our image data only. This is because CNNs have the ability to relate features locality which makes sense for image data, but does not really help with low dimensionality we see in the point reference data.  

\subsection{Implementation}
We implemented our CNN using a GPU version of TensorFlow. Similar to homework 3, we had an input layer, multiple convolution and pooling layers, a hidden layers, and an output layer.(See Figure ....) 

For the input layer we took each channel(RGB) of in the image and normalized the pixel values such that they could only lay within -0.5 to 0.5. The size of our input layer corresponded to the number of pixels times the number of channels. For the output layer, we used 12 nodes. There were three nodes that represented the 3D position of the object and nine nodes that represented the objects orientation as a direction cosine matrix. We did not apply any activation function the output layer since we were attempting to perform regression and not classification. 

The number of layers, nodes, filter size and stride was tunable hyperparameters for the convolution, pooling layers. For the standard hidden layers, the number of layers and nodes were a tunable parameter. The convolutional and hiddens layer both used Relu activation functions, while the pooling layers used maximum operation as activation functions. 



\subsection{Training}
For training the data, we used a standard L2 Loss function as well the same gradient descent method used in homework 3. 

Since we were not totally sure the best hyperparameters to use for the data set, we decided to implement version of coordinate ascent to attempt to maximize performance. 
In each iteration of coordinate descent we ran through the 

\subsection{Performance on Raw Image Data}


\section{Summary}


\begin{thebibliography}{10}
	
	\bibitem{Robust_PNP} 
	S. Li, C. Xu and M. Xie. \textit{A Robust O(n) Solution to the Perspective-n-Point Problem}, in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, p. 1444-1450, 2012.
	
	\bibitem{EPnP} 
	V. Lepetit, F. Moreno-Noguer and P. Fua. \textit{EPnP: An Accurate O(n) Solution to the PnP Problem}, in International Journal Of Computer Vision, vol. 81, p. 155-166, 2009.
	
		\bibitem{vive_image} 
	Image Source: \\\texttt{https://i.ytimg.com/vi/J54dotTt7k0/maxresdefault.jpg}
	
	
	\bibitem{oculus_image} 
	Image Source: \\\texttt{http://www.roadtovr.com/wp-content/uploads/2014/01/oculus-rift-crystal-cove.jpeg}
\end{thebibliography}

% For peer review papers, you can put extra information on the cover
% page as needed:
 \ifCLASSOPTIONpeerreview
 \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
 \fi



% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




   
\end{document}


%Example figure syntax

%\begin{figure}[h]
%\centering
%  \noindent
%  \centering{\hspace{-8 ex} \includegraphics[width=3in]{layers2}}
%  \caption{Boreholes and soil profile with measurements depths.}\label{Fig1Label}

%\end{figure}


%Example equation syntax
%\begin{equation}\label{eq:wavenum}
%k(z) = 2\pi f\sqrt{\epsilon_c(z)\mu} = \sqrt{(k^{mode_n}_\rho)^2 + %(k^{mode_n}_z)^2(z)},
%\end{equation}





